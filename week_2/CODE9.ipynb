{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CODE9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJyVpgSxHt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlEUplvoxKAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Y9Va-xxMXG",
        "colab_type": "code",
        "outputId": "93ee364a-e3b3-4b05-eaf0-26c11ff15550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[8])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f107531d160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxDZxPhhxOgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzMqbTnxQQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LdYiW6ixR9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train[:10]\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFR0F9j0xVp2",
        "colab_type": "code",
        "outputId": "4502516f-4f52-42d5-f7e7-051eb7c425da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpXf4YQxXRm",
        "colab_type": "code",
        "outputId": "52b6a546-a0f6-4e43-a703-fa1d0e4e8dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        " \n",
        "model.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1), use_bias=False)) #26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu', use_bias=False)) #24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(8, 1, 1, activation='relu', use_bias=False)) #22\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#11\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))#9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))#7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))#5\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))#3\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(10, 4, 4, use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1..., use_bias=False)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (1, 1), activation=\"relu\", use_bias=False)`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_25 (Conv2D)           (None, 26, 26, 10)        90        \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 24, 24, 20)        1800      \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 24, 24, 20)        80        \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 24, 24, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 24, 24, 8)         160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 10, 10, 16)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 8, 8, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 6, 6, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 4, 4, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 1, 1, 10)          2560      \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 13,090\n",
            "Trainable params: 12,882\n",
            "Non-trainable params: 208\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4), use_bias=False)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2IicGJ4x3Be",
        "colab_type": "code",
        "outputId": "0583dc00-c21d-4067-d5d4-ac9a25d7bba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "history = []\n",
        "scores = []\n",
        "for i in range(4):\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "  h = model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "  history.append(h)\n",
        "  score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "  scores.append(score)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 14s 240us/step - loss: 0.5482 - acc: 0.8441 - val_loss: 0.1005 - val_acc: 0.9814\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.2600 - acc: 0.9219 - val_loss: 0.0597 - val_acc: 0.9869\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.2036 - acc: 0.9384 - val_loss: 0.0656 - val_acc: 0.9835\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.1741 - acc: 0.9461 - val_loss: 0.0369 - val_acc: 0.9916\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.1562 - acc: 0.9476 - val_loss: 0.0396 - val_acc: 0.9886\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.1400 - acc: 0.9513 - val_loss: 0.0323 - val_acc: 0.9909\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 10s 168us/step - loss: 0.1340 - acc: 0.9517 - val_loss: 0.0295 - val_acc: 0.9916\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 11s 175us/step - loss: 0.1254 - acc: 0.9530 - val_loss: 0.0285 - val_acc: 0.9918\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.1218 - acc: 0.9528 - val_loss: 0.0235 - val_acc: 0.9934\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.1178 - acc: 0.9539 - val_loss: 0.0257 - val_acc: 0.9920\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.1140 - acc: 0.9548 - val_loss: 0.0232 - val_acc: 0.9929\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.1106 - acc: 0.9554 - val_loss: 0.0200 - val_acc: 0.9937\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.1060 - acc: 0.9562 - val_loss: 0.0206 - val_acc: 0.9932\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.1058 - acc: 0.9555 - val_loss: 0.0231 - val_acc: 0.9918\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.1029 - acc: 0.9561 - val_loss: 0.0205 - val_acc: 0.9932\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.1020 - acc: 0.9550 - val_loss: 0.0237 - val_acc: 0.9926\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0991 - acc: 0.9560 - val_loss: 0.0200 - val_acc: 0.9940\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0990 - acc: 0.9557 - val_loss: 0.0222 - val_acc: 0.9928\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0964 - acc: 0.9575 - val_loss: 0.0211 - val_acc: 0.9931\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0961 - acc: 0.9569 - val_loss: 0.0206 - val_acc: 0.9933\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 15s 253us/step - loss: 0.1148 - acc: 0.9522 - val_loss: 0.0263 - val_acc: 0.9922\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 11s 181us/step - loss: 0.1061 - acc: 0.9534 - val_loss: 0.0389 - val_acc: 0.9896\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 11s 180us/step - loss: 0.0990 - acc: 0.9553 - val_loss: 0.0251 - val_acc: 0.9918\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0955 - acc: 0.9558 - val_loss: 0.0228 - val_acc: 0.9926\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0947 - acc: 0.9558 - val_loss: 0.0208 - val_acc: 0.9942\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0921 - acc: 0.9566 - val_loss: 0.0196 - val_acc: 0.9941\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 11s 177us/step - loss: 0.0926 - acc: 0.9552 - val_loss: 0.0238 - val_acc: 0.9930\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0882 - acc: 0.9584 - val_loss: 0.0186 - val_acc: 0.9936\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0850 - acc: 0.9601 - val_loss: 0.0207 - val_acc: 0.9937\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.0858 - acc: 0.9577 - val_loss: 0.0204 - val_acc: 0.9935\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0855 - acc: 0.9582 - val_loss: 0.0198 - val_acc: 0.9938\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0852 - acc: 0.9576 - val_loss: 0.0186 - val_acc: 0.9942\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0846 - acc: 0.9579 - val_loss: 0.0197 - val_acc: 0.9939\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0818 - acc: 0.9596 - val_loss: 0.0194 - val_acc: 0.9939\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0829 - acc: 0.9580 - val_loss: 0.0172 - val_acc: 0.9948\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0825 - acc: 0.9597 - val_loss: 0.0183 - val_acc: 0.9934\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.0815 - acc: 0.9589 - val_loss: 0.0201 - val_acc: 0.9943\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0806 - acc: 0.9589 - val_loss: 0.0189 - val_acc: 0.9942\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0819 - acc: 0.9583 - val_loss: 0.0173 - val_acc: 0.9947\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0794 - acc: 0.9605 - val_loss: 0.0168 - val_acc: 0.9948\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 15s 254us/step - loss: 0.0996 - acc: 0.9546 - val_loss: 0.0307 - val_acc: 0.9901\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.0927 - acc: 0.9561 - val_loss: 0.0237 - val_acc: 0.9924\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0900 - acc: 0.9562 - val_loss: 0.0307 - val_acc: 0.9925\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.0870 - acc: 0.9572 - val_loss: 0.0204 - val_acc: 0.9938\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.0850 - acc: 0.9568 - val_loss: 0.0213 - val_acc: 0.9936\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.0845 - acc: 0.9574 - val_loss: 0.0239 - val_acc: 0.9928\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.0808 - acc: 0.9605 - val_loss: 0.0194 - val_acc: 0.9938\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.0790 - acc: 0.9591 - val_loss: 0.0212 - val_acc: 0.9936\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0790 - acc: 0.9581 - val_loss: 0.0176 - val_acc: 0.9948\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0790 - acc: 0.9595 - val_loss: 0.0170 - val_acc: 0.9956\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0806 - acc: 0.9591 - val_loss: 0.0198 - val_acc: 0.9946\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0782 - acc: 0.9597 - val_loss: 0.0194 - val_acc: 0.9945\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0774 - acc: 0.9600 - val_loss: 0.0169 - val_acc: 0.9952\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0766 - acc: 0.9599 - val_loss: 0.0187 - val_acc: 0.9947\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0770 - acc: 0.9596 - val_loss: 0.0211 - val_acc: 0.9942\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0759 - acc: 0.9604 - val_loss: 0.0182 - val_acc: 0.9945\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.0757 - acc: 0.9595 - val_loss: 0.0187 - val_acc: 0.9942\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0766 - acc: 0.9612 - val_loss: 0.0203 - val_acc: 0.9946\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.0750 - acc: 0.9603 - val_loss: 0.0158 - val_acc: 0.9953\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 0.0740 - acc: 0.9604 - val_loss: 0.0171 - val_acc: 0.9948\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0931 - acc: 0.9543 - val_loss: 0.0258 - val_acc: 0.9920\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 11s 182us/step - loss: 0.0886 - acc: 0.9558 - val_loss: 0.0224 - val_acc: 0.9932\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.0855 - acc: 0.9568 - val_loss: 0.0200 - val_acc: 0.9941\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 11s 183us/step - loss: 0.0837 - acc: 0.9581 - val_loss: 0.0206 - val_acc: 0.9943\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.0781 - acc: 0.9596 - val_loss: 0.0185 - val_acc: 0.9952\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.0780 - acc: 0.9597 - val_loss: 0.0203 - val_acc: 0.9940\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 11s 180us/step - loss: 0.0778 - acc: 0.9592 - val_loss: 0.0173 - val_acc: 0.9948\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 11s 181us/step - loss: 0.0791 - acc: 0.9597 - val_loss: 0.0183 - val_acc: 0.9951\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 11s 181us/step - loss: 0.0757 - acc: 0.9602 - val_loss: 0.0190 - val_acc: 0.9948\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0762 - acc: 0.9602 - val_loss: 0.0183 - val_acc: 0.9950\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0751 - acc: 0.9599 - val_loss: 0.0189 - val_acc: 0.9942\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0760 - acc: 0.9600 - val_loss: 0.0186 - val_acc: 0.9946\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 11s 180us/step - loss: 0.0739 - acc: 0.9598 - val_loss: 0.0173 - val_acc: 0.9947\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 12s 208us/step - loss: 0.0734 - acc: 0.9611 - val_loss: 0.0204 - val_acc: 0.9943\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.0736 - acc: 0.9610 - val_loss: 0.0176 - val_acc: 0.9943\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.0745 - acc: 0.9602 - val_loss: 0.0174 - val_acc: 0.9954\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 11s 181us/step - loss: 0.0734 - acc: 0.9610 - val_loss: 0.0170 - val_acc: 0.9949\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 11s 183us/step - loss: 0.0746 - acc: 0.9598 - val_loss: 0.0176 - val_acc: 0.9949\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 11s 182us/step - loss: 0.0714 - acc: 0.9621 - val_loss: 0.0185 - val_acc: 0.9948\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0721 - acc: 0.9605 - val_loss: 0.0164 - val_acc: 0.9943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTiZSUK-8LtQ",
        "colab_type": "code",
        "outputId": "5c62826f-5714-458e-a868-9df18e3e7fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# find out the highest val_acc in the 4 runs of the model\n",
        "max_acc = [(max(history[i].history['val_acc']), i) for i in range(4)]\n",
        "print(max_acc)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0.994, 0), (0.9948, 1), (0.9956, 2), (0.9954, 3)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLxlW9ufyQiO",
        "colab_type": "code",
        "outputId": "e7dda141-722b-4f91-abf1-3042ee93d555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(scores)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.02059032255401835, 0.9933], [0.016840569843596314, 0.9948], [0.01714658085524061, 0.9948], [0.0163537852096797, 0.9943]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-o2TFLlpl9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}